# -*- coding: utf-8 -*-
"""QuestionAnsweringSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qouz1RUcapfM-mB5x-8S76wwNkJ11o3w

### **Implementing a Question Answer System using Neural Networks**



1.   Chris Mary Benson(2020A7PS0027U)
2.   Vanshika Sangtani(2020A7PS0031U)

## **Importing Packages**
"""

import keras
from keras.models import Sequential, Model
from keras.layers import Embedding
from keras.layers import Input, Activation, Dense, Permute, Dropout
from keras.layers import add, dot, concatenate
from keras.layers import LSTM
from keras.preprocessing.sequence import pad_sequences
from keras import backend as K

import numpy as np
import re

import pandas as pd

"""## **Importing** **SQuAD** **dataset**"""

!pip install datasets
from datasets import load_dataset

# Load the SQuAD dataset
squad_dataset = load_dataset("squad")

qa_data = squad_dataset["train"]

valid_data = squad_dataset["validation"]

qa_data

qa_data["context"]

qa_data['answers']

"""## **Pre**-**processing** **the** **data**"""

lookup = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'
# to check for valid characters in the data to convert to lowercase
def in_white_list(_word):
    valid_word = False
    for char in _word:
        if char in lookup:
            valid_word = True
            break

    if valid_word is False:
        return False

    return True

def count_words(string):
    words = string.split()
    return len(words)

def pre_process_data(qa_data,max_data_count,max_context_seq_length,max_question_seq_length,max_target_seq_length):
  data = list()
  for instance in qa_data:
      context = instance['context']
      context_wid_list = [w.lower() for w in nltk.word_tokenize(context) if in_white_list(w)]
      print(context_wid_list)
      new_context = " ".join(context_wid_list)
      if len(context_wid_list) > max_context_seq_length:
                continue
      question = instance['question']
      print(question)
      question_wid_list = [w.lower() for w in nltk.word_tokenize(question) if in_white_list(w)]
      new_question = " ".join(question_wid_list)
      if len(question_wid_list) > max_question_seq_length:
                    continue
      answers = instance['answers']
      #print(answers['text'])
      answ = answers['text']
      word_count = count_words(answ[0])
      #print("Number of words:", word_count)
      if word_count == 1:
        for ans in answ:
          answer_wid_list = [w.lower() for w in nltk.word_tokenize(ans) if in_white_list(w)]
          new_answer = " ".join(answer_wid_list)
          print(new_answer)
          if len(answer_wid_list) > max_target_seq_length:
                  continue
        if len(data) < max_data_count:
                data.append((new_context, new_question, new_answer))
  return data

import nltk
nltk.download('punkt')

#preprocessing the training data
max_data_count = 8000
max_context_seq_length = 300
max_question_seq_length = 60
max_target_seq_length = 50

data = pre_process_data(qa_data, max_data_count, max_context_seq_length, max_question_seq_length, max_target_seq_length)

print("Total Q&A data size", len(data))

len(data)

data[400]

#train-test split
import random
from sklearn.model_selection import train_test_split

random.seed(42)
s_data = random.sample(data, len(data))

squad_data_train, squad_data_test = train_test_split(s_data, test_size = 0.2, random_state = 42)

len(squad_data_train), len(squad_data_test)

squad_data = []
for context, query, answer in squad_data_train:
  context_list = [w for w in nltk.word_tokenize(context)]
  question_list = [w for w in nltk.word_tokenize(query)]
  squad_data.append((context_list, question_list, answer))

squad_test_data = []
for context, query, answer in squad_data_test:
  context_list = [w for w in nltk.word_tokenize(context)]
  question_list = [w for w in nltk.word_tokenize(query)]
  squad_test_data.append((context_list, question_list, answer))

squad_data[200]

squad_test_data[200]

"""## **Vectorizing the context, query, answer**"""

def vectorize(data, word_idx, story_maxlen, query_maxlen):
    X = []
    Xq = []
    Y = []
    for context, query, answer in data:
        x = [word_idx[w] for w in context]
        xq = [word_idx[w] for w in query]
        y = np.zeros(len(word_idx) + 1)
        y[word_idx[answer]] = 1
        X.append(x)
        Xq.append(xq)
        Y.append(y)
    return (pad_sequences(X, maxlen=context_maxlen),
            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))

"""## **Creating a Vocabulary**"""

vocab = set()
for context, q, answer in squad_data + squad_test_data:
    vocab |= set(context + q + [answer])
    print([answer])
vocab = sorted(vocab)

# Reserve 0 for masking via pad_sequences
vocab_size = len(vocab) + 1
context_maxlen = max(map(len, (x for x, _, _ in squad_data + squad_test_data)))
query_maxlen = max(map(len, (x for _, x, _ in squad_data + squad_test_data)))

len(vocab)

context_maxlen, query_maxlen

print('Vocab size:', vocab_size, 'unique words')
print('Context max length:', context_maxlen, 'words')
print('Query max length:', query_maxlen, 'words')
print('Number of training stories:', len(squad_data))
print('Number of test stories:', len(squad_test_data))
print('-')
print('(Context, query, answer):')
print(squad_data_train[0])
print('-')
print('Vectorizing the word sequences...')

word_idx = dict((c, i + 1) for i, c in enumerate(vocab))
idx_word = dict((i+1, c) for i,c in enumerate(vocab))
inputs_train, queries_train, answers_train = vectorize(squad_data,
                                                               word_idx,
                                                               context_maxlen,
                                                               query_maxlen)
inputs_test, queries_test, answers_test = vectorize(squad_test_data,
                                                            word_idx,
                                                            context_maxlen,
                                                            query_maxlen)

inputs_train.shape, queries_train.shape, answers_train.shape

print('inputs: integer tensor of shape (samples, max_length)')
print('inputs_train shape:', inputs_train.shape)
print('inputs_test shape:', inputs_test.shape)
print('-')
print('queries: integer tensor of shape (samples, max_length)')
print('queries_train shape:', queries_train.shape)
print('queries_test shape:', queries_test.shape)
print('-')
print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')
print('answers_train shape:', answers_train.shape)
print('answers_test shape:', answers_test.shape)

train_epochs = 200
batch_size = 32
lstm_size = 64

"""## **Bidirectional LSTM Model**"""

from keras.layers import Bidirectional

input_sequence = Input((context_maxlen,))
question = Input((query_maxlen,))

print('Input sequence:', input_sequence)
print('Question:', question)

input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,
                              output_dim=64))
input_encoder_m.add(Dropout(0.3))

input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,
                              output_dim=query_maxlen))
input_encoder_c.add(Dropout(0.3))

question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=64,
                               input_length=query_maxlen))
question_encoder.add(Dropout(0.3))

input_encoded_m = input_encoder_m(input_sequence)
print('Input encoded m', input_encoded_m)
input_encoded_c = input_encoder_c(input_sequence)
print('Input encoded c', input_encoded_c)
question_encoded = question_encoder(question)
print('Question encoded', question_encoded)

match = dot([input_encoded_m, question_encoded], axes=(2, 2))
print(match.shape)
match = Activation('softmax')(match)
print('Match shape', match)

response = add([match, input_encoded_c])
response = Permute((2, 1))(response)
print('Response shape', response)

answer = concatenate([response, question_encoded])
print('Answer shape', answer)

answer = Bidirectional(LSTM(lstm_size, return_sequences=True))(answer)
answer = Dropout(0.5)(answer)
answer = Bidirectional(LSTM(lstm_size))(answer)
answer = Dropout(0.5)(answer)
answer = Dense(vocab_size)(answer)
answer = Activation('softmax')(answer)

model = Model([input_sequence, question], answer)
model.compile(optimizer='adam', loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True)

model.fit([inputs_train, queries_train], answers_train, batch_size, train_epochs, validation_data=([inputs_test, queries_test], answers_test))

from google.colab import drive
drive.mount('/content/drive')

model.save('/content/drive/MyDrive/BILSTM_QnA_model'+'/qna_model.h5')

from keras.models import load_model
new_model = load_model('/content/drive/MyDrive/BILSTM_QnA_model/qna_model.h5')

"""## **Evaluation Metrics**"""

from collections import Counter

def normalize_answer(answer):
    return answer.lower().strip()

def exact_match_score(prediction, ground_truth):
    return int(normalize_answer(prediction) == normalize_answer(ground_truth))

def f1_score(prediction, ground_truth):
    if len(prediction) == 0:
      p = -1
      r = -1
      f1 = -1
      print(" ")
    else:
      if not ground_truth:
          p = -1
          r = -1
          f1 = -1
          return 0, 0, 0

      common = Counter(normalize_answer(prediction)) & Counter(normalize_answer(ground_truth))
      num_common = sum(common.values())

      p = num_common / len(normalize_answer(prediction))
      print("Precision: ", p)
      r = num_common / len(normalize_answer(ground_truth))
      print("Recall: ", r)

      if p + r == 0:
          return 0, 0, 0

      f1 = (2 * p * r) / (p + r)
    return p, r, f1

"""## **Testing the model**"""

precision_list = []
recall_list = []
f1_list = []
EM_scores = []
j = 0

for i in range(0, 200):
        if j>0:
          current_inp = squad_data[i]
          j += 1
        else:
          current_inp = squad_test_data[i]
          j -= 1
        #current_inp = squad_test_data[i]
        current_context, current_query, current_answer = vectorize([current_inp], word_idx, context_maxlen, query_maxlen)
        current_prediction = model.predict([current_context, current_query])
        current_prediction = idx_word[np.argmax(current_prediction)]
        #print(current_inp[0])
        print("Context: ", ' '.join(current_inp[0]))
        print("Question: ", ' '.join(current_inp[1]))
        print('Prediction:', current_prediction, '| Actual:', current_inp[2])
        em = exact_match_score(current_prediction, current_inp[2])
        EM_scores.append(em)
        print("Exact Match (EM) score: ", em)
        p, r, f1 = f1_score(current_prediction, current_inp[2])
        if p == -1:
          print("Model could not predict an answer")
        else:
          print("f1 score: ", f1)
          f1_list.append(f1)
          precision_list.append(p)
          recall_list.append(r)
        print("-----------------------------------------------------------------------------------------")

for i in range(0, 100):
        current_inp = squad_data[i]
        #current_inp = squad_test_data[i]
        current_context, current_query, current_answer = vectorize([current_inp], word_idx, context_maxlen, query_maxlen)
        current_prediction = model.predict([current_context, current_query])
        current_prediction = idx_word[np.argmax(current_prediction)]
        #print(current_inp[0])
        print("Context: ", ' '.join(current_inp[0]))
        print("Question: ", ' '.join(current_inp[1]))
        print('Prediction:', current_prediction, '| Actual:', current_inp[2])
        em = exact_match_score(current_prediction, current_inp[2])
        #EM_scores.append(em)
        print("Exact Match (EM) score: ", em)
        p, r, f1 = f1_score(current_prediction, current_inp[2])
        if p == -1:
          print("Model could not predict an answer")
        else:
          print("f1 score: ", f1)
          #f1_list.append(f1)
          #precision_list.append(p)
          #recall_list.append(r)
        print("-----------------------------------------------------------------------------------------")

p_avg = 0
r_avg = 0
f1_avg = 0
EM_avg = 0
for i in range(0, len(precision_list)):
  f1_avg += f1_list[i]
  p_avg += precision_list[i]
  r_avg += recall_list[i]
for i in range(0, len(EM_scores)):
  EM_avg += EM_scores[i]
print("F1 score: ", f1_avg/len(f1_list))
print("Precision: ", p_avg/len(precision_list))
print("Recall :", r_avg/len(recall_list))
print("EM score: ", EM_avg/len(EM_scores))

precision_list

"""## **Plotting the graphs for the evaluation metrics**"""

import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.25
index = np.arange(len(precision_list))

bar1 = ax.bar(index, precision_list, width=bar_width, label='Precision', color='blue')

ax.set_xlabel('Candidates')
ax.set_ylabel('Precision')
ax.set_title('Precision')
ax.set_xticks(index + bar_width)
ax.legend()

plt.show()

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.25
index = np.arange(len(recall_list))

bar2 = ax.bar(index + bar_width, recall_list, width=bar_width, label='Recall', color='orange')

ax.set_xlabel('Candidates')
ax.set_ylabel('Recall')
ax.set_title('Recall')
ax.set_xticks(index + bar_width)
ax.legend()

plt.show()

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.25
index = np.arange(len(f1_list))

bar3 = ax.bar(index + 2 * bar_width, f1_list, width=bar_width, label='F1 Score', color='green')

ax.set_xlabel('Candidates')
ax.set_ylabel('F1-score')
ax.set_title('F1-score')
ax.set_xticks(index + bar_width)
ax.legend()

plt.show()